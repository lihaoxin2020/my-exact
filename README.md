# Improving LLM Web Navigation with Direct Preference Optimization Over Test-Time Scaling Responses

This codebase is built on the ExACT project, which presents R-MCTS and Exploratory Learning for building o1-like models for agentic applications. For more details about the project, refer to [ORIGINAL_README.md](ORIGINAL_README.md).

See the project report [here](<CPSC 577 Submission/17_RL_For_Web_Navigation.pdf>) and checklist [here](<CPSC 577 Submission/Reproducibility checklis - Google Docs.pdf>). (**Note**: we recommend download to have higher resolution.)

## Installation and Setup

### 1. WebArena Environment Setup

First, set up the WebArena environment by following the official repository instructions:

1. Clone the WebArena repository:
   ```bash
   git clone https://github.com/web-arena-x/webarena.git
   cd webarena
   
   # create a python env with conda or venv where python=3.10
   pip install -r requirements.txt
   playwright install
   pip install -e .
   ```

2. Follow the official WebArena setup guide to set up the website environments using either AWS or Docker.
   - For Docker setup, follow the instructions in the WebArena repository's README to set up the local Docker environment.
   - Make sure all the websites (Classifieds, Shopping, Reddit, etc.) are running properly.

### 2. Project Setup

Once WebArena is set up, proceed with setting up the ExACT project:

1. Clone the ExACT repository:
   ```bash
   git clone https://github.com/lihaoxin2020/my-exact.git
   cd my-exact
   ```

2. Install the dependencies from requirements.txt:
   ```bash
   pip install -r requirements.txt
   ```

   This will install all required packages specifically for this project. 

### 3. Dependencies and Versions

The project requires the following dependencies with their specified versions:

- Python 3.10
- absl-py==1.4.0 - Google's Abseil Python libraries
- rouge-score==0.1.2 - Python implementation of ROUGE for text evaluation
- langchain==0.0.267 - Framework for developing applications with LLMs
- langchain-community==0.0.10 - Community extensions for LangChain
- langchain-openai==0.0.2.post1 - OpenAI integration for LangChain
- langchain-huggingface==0.0.6 - Hugging Face integration for LangChain
- torch==2.1.0 - PyTorch deep learning framework
- azure-storage-blob==12.18.3 - Azure Storage Blob client library
- azure-identity==1.14.1 - Azure Identity client library

Note that additional dependencies may be required from the WebArena project itself. The versions above represent the minimum compatible versions tested with this project.

### 4. Configuration and Running

After installing the dependencies, follow the remaining setup instructions from the original documentation:

1. Export the necessary environment variables:
   ```bash
   ##### VLM providers
   export OPENAI_API_KEY=sk-xxx
   export OPENAI_ORGANIZATION=org-xxx
   export HF_TOKEN=hf_xxx
   # optional keys (other providers)
   export AZURE_OPENAI_API_BASE=https://xxx
   export AZURE_TOKEN_PROVIDER_API_BASE=https://xxx

   ##### (V)WA Web URLs
   export DATASET="<visualwebarena or webarena>"
   export CLASSIFIEDS="<your_classifieds_domain>:9980"
   export CLASSIFIEDS_RESET_TOKEN="4b61655535e7ed388f0d40a93600254c"  # Default reset token for classifieds site, change if you edited its docker-compose.yml
   export SHOPPING="<your_shopping_site_domain>:7770"
   export REDDIT="<your_reddit_domain>:9999"
   export WIKIPEDIA="<your_wikipedia_domain>:8888"
   export SHOPPING_ADMIN="<your_e_commerce_cms_domain>:7780/admin"
   export GITLAB="<your_gitlab_domain>:8023"
   export MAP="<your_map_domain>:3000"
   export HOMEPAGE="<your_homepage_domain>:4399"
   ```

2. Generate the task configurations:
   ```bash
   # generate WA task configs
   export DATASET=webarena
   python runners/utils/generate_test_configs.py
   ```

## Evaluation Metrics and Methods

### Task Success Rate
The primary evaluation metric used is task success rate, measured across various WebArena environments. Tasks are considered successful if they meet the specific goal criteria defined in the WebArena benchmark.

### Token Usage Analysis
We tracked token usage across models to measure efficiency:
- Input tokens: Number of tokens in prompts and context
- Output tokens: Number of tokens generated by models
- Total tokens: Sum of input and output tokens

### Trajectory Analysis
We analyzed agent trajectories using several metrics:
- Action efficiency: Number of actions needed to complete a task
- Tree exploration metrics: For R-MCTS agents, we analyzed tree depth and breadth
- Backtracking frequency: How often the agent needed to revisit previous states

### Statistical Methods
- For comparative analysis between models, we used paired t-tests to determine statistical significance
- Bootstrap resampling was used to estimate confidence intervals
- Performance was measured across multiple runs to ensure robustness

## Running Experiments

For instructions on how to run experiments, including quickstart examples and how to parallelize evaluations, please refer to the [ORIGINAL_README.md](ORIGINAL_README.md). 


### Extracting and Processing Trajectories
To process WebArena trajectories:
```bash
python trajectory_collection/extract_successful_react_trajectories.py \
  --input_dir="./trajectories/raw/" \
  --output_dir="./trajectories/processed/"
```

### Analyzing Trajectories
To visualize and analyze trajectories:
```bash
python analysis/visualize_trajectories.py \
  --trajectory_dir="./trajectories/" \
  --models="o4-mini,llama31-8b,ft-llama31" \
  --output_dir="./analysis/visualizations/"

python analysis/analyze_trajectories_comparison.py \
  --trajectory_dir="./trajectories/" \
  --models="o4-mini,llama31-8b,ft-llama31" \
  --output_file="./analysis/comparison_results.json"

python analysis/analyze_tokens.py \
  --trajectory_dir="./trajectories/" \
  --models="o4-mini,llama31-8b" \
  --output_file="./analysis/token_usage.json"
```

To collect and match performance data with trajectory files:
```bash
python analysis/collect_performance_trajectory_files.py \
  --performance_dir="./results/performance/" \
  --trajectory_dir="./trajectories/" \
  --output_dir="./analysis/matched/"
```

To filter trajectories based on specific criteria:
```bash
python analysis/filter_performances.py \
  --input_file="./results/performance/all_performances.json" \
  --output_file="./results/performance/filtered_performances.json" \
  --filter_criteria="success=True"

python analysis/filter_llama3_trajectories.py \
  --input_dir="./trajectories/llama31-8b/" \
  --output_dir="./trajectories/llama31-8b-filtered/" \
  --filter_criteria="max_steps=25"
```

### Creating Training Datasets
The SFT training dataset was created from 338 successful o4-mini trajectories (with 38 validation examples):

```bash
python dataset_creation/convert_trajectories.py \
  --input_dir="./trajectories/o4-mini/" \
  --output_dir="./training_data/sft/" \
  --format="sft"
```

This creates the training data files for the SFT model.

The preference data for DPO can be created using similar commands with the DPO format option.

### Training Models
SFT training was performed for 10 epochs with batch size 8 and learning rate 0.00002 using a linear scheduler:

```bash
python training/train_model.py \
  --model_name="meta-llama/Meta-Llama-3.1-8B-Instruct" \
  --train_file="./training_data/sft/sft_data.jsonl" \
  --valid_file="./training_data/sft/valid.jsonl" \
  --output_dir="./models/sft-llama31-8b" \
  --epochs=10 \
  --batch_size=8 \
  --learning_rate=0.00002 \
  --scheduler="linear" \
  --together_api_key="your_together_api_key"
```

To fix DPO data format issues when needed:
```bash
python training/fix_dpo_format_chat.py \
  --input_file="./training_data/dpo/train_raw.json" \
  --output_file="./training_data/dpo/train.json"
```

Running python training/train_model.py will set up TogetherAI to run the model for you. 

From this, you will need to set up your TogetherAI account and API key to run the training jobs. You can then deploy the model on TogetherAI and then create a script for training and evaluation by replacing the .sh files. 

